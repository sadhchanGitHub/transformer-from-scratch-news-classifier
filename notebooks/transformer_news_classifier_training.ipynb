{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45687888-cb1b-4b11-b039-2cf9e6b41c1d",
   "metadata": {},
   "source": [
    "## Acknowledgments:\n",
    "\"\"\"\n",
    "The core concepts and architectural patterns implemented here were learned from and inspired by several excellent educational resources, including Jay Alammar's \"The Illustrated Transformer\", Andrej Karpathy's \"Let's build GPT\", and Josh Starmer's course on DeepLearning.AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb446ae-6970-4216-8531-de21fb4b1bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries and modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "\n",
    "# Core PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# TorchText for NLP\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Standard Python Libraries\n",
    "import math\n",
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "# Utilities\n",
    "import portalocker # For safe data downloading\n",
    "\n",
    "# custom modules\n",
    "import sys\n",
    "import os\n",
    "# Go up one level from 'notebooks' to the project root\n",
    "sys.path.append(os.path.abspath('..')) \n",
    "\n",
    "\n",
    "from model import TransformerClassifier\n",
    "from utils import evaluate\n",
    "import config\n",
    "\n",
    "\n",
    "print(\"✅ All libraries and modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153eec7-5573-41f7-b7de-3106b2ca07f9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54965e6c-13dc-4868-b68f-c965a63e50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part1: Data Preparation\n",
    "\n",
    "def getSrcdata(full_dataset=False, sample_size=1000, cache_dir=\"./ag_news_cache\"):\n",
    "#def getSrcdata(cache_dir=\"./ag_news_cache\"):\n",
    "    # Set up tokenizer\n",
    "    #tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def yield_tokens(data_iter):\n",
    "        for _, text in data_iter:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "    # Force single-thread token processing\n",
    "    torch.set_num_threads(1)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    lock_path = os.path.join(cache_dir, \"download.lock\")\n",
    "\n",
    "    # Safe cache access with portalocker\n",
    "    with open(lock_path, \"w\") as lock_file:\n",
    "        portalocker.lock(lock_file, portalocker.LOCK_EX)\n",
    "        try:\n",
    "            train_iter, test_iter = AG_NEWS(root=cache_dir, split=('train', 'test'))\n",
    "        finally:\n",
    "            portalocker.unlock(lock_file)\n",
    "\n",
    "        # Take only first `sample_size` training/test samples\n",
    "    if full_dataset:\n",
    "        print(\"Loading FULL dataset...\")\n",
    "        train_sample = list(train_iter)\n",
    "        test_sample = list(test_iter)\n",
    "    else:\n",
    "        print(f\"Loading SAMPLE of {sample_size}...\")\n",
    "        train_sample = list(islice(train_iter, sample_size))\n",
    "        test_sample = list(islice(test_iter, sample_size))\n",
    "        \n",
    "    print(f\"Training sample size: {len(train_sample)}\")\n",
    "    print(f\"Test sample size: {len(test_sample)}\")\n",
    "\n",
    "    # Build vocab from first `sample_size` training samples\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_sample),\n",
    "        specials=[\"<unk>\", \"<pad>\"]\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return train_sample, test_sample, vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ae9eb3-c963-4454-bcda-e8fe5a8ddc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part2: Define a collate function to process batches\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        # --- FIX THE LABEL HERE, ONCE AND FOR ALL ---\n",
    "        labels_list.append(_label - 1) \n",
    "        processed_text = torch.tensor([vocab[token] for token in tokenizer(_text)], dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
    "    padded_text = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    return padded_text, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d94595-d611-46e9-99e6-534254c4eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer defined globally.\n"
     ]
    }
   ],
   "source": [
    "## Part3: Define the tokenizer (should be global)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "print(\"✅ Tokenizer defined globally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115f46b3-0417-48aa-8cf2-a0fb08778375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FULL dataset...\n",
      "Training sample size: 120000\n",
      "Test sample size: 7600\n",
      "✅ Vocab saved to ../models/newsclassification_vocab.pth\n",
      "✅ Train and Test DataLoaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Part5: get data now from above function\n",
    "\n",
    "# 1. Get the raw data and vocab\n",
    "train_sample, test_sample, vocab = getSrcdata(full_dataset=config.FULL_DATASET) # Using a sample for speed\n",
    "\n",
    "# --- SAVE THE VOCAB ARTIFACT ---\n",
    "torch.save(vocab, config.VOCAB_SAVE_PATH)\n",
    "print(f\"✅ Vocab saved to {config.VOCAB_SAVE_PATH}\")\n",
    "\n",
    "# 2. Define PAD_IDX\n",
    "PAD_IDX = vocab['<pad>']\n",
    "\n",
    "# 3. Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_sample, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_sample, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(\"✅ Train and Test DataLoaders created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b0179d-7582-46a2-9463-f29204bad1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "VOCAB_SIZE = len(vocab) # This needs to be calculated after data loading\n",
    "\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=config.D_MODEL,\n",
    "    num_heads=config.NUM_HEADS,\n",
    "    num_layers=config.NUM_LAYERS,\n",
    "    d_ff=config.D_FF,\n",
    "    num_classes=config.NUM_CLASSES\n",
    ").to(config.DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91eaea4-8512-4580-9703-e8e5bcae3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting the first batch from the DataLoader ---\n",
      "Batch token shape: torch.Size([64, 83])\n",
      "Batch label shape: torch.Size([64])\n",
      "\n",
      "Unique labels in this batch: tensor([0, 1, 2, 3])\n",
      "Minimum label in this batch: 0\n",
      "Maximum label in this batch: 3\n",
      "\n",
      "✅ SUCCESS: The labels in this batch are correct.\n"
     ]
    }
   ],
   "source": [
    "# --- DATALOADER DEFINITIVE TEST ---\n",
    "print(\"--- Inspecting the first batch from the DataLoader ---\")\n",
    "\n",
    "# Get one batch of data from the loader\n",
    "try:\n",
    "    first_batch_tokens, first_batch_labels = next(iter(train_dataloader))\n",
    "\n",
    "    # Perform the sanity check on THIS BATCH\n",
    "    unique_labels = first_batch_labels.unique()\n",
    "    min_label = first_batch_labels.min().item()\n",
    "    max_label = first_batch_labels.max().item()\n",
    "    \n",
    "    print(f\"Batch token shape: {first_batch_tokens.shape}\")\n",
    "    print(f\"Batch label shape: {first_batch_labels.shape}\")\n",
    "    print(f\"\\nUnique labels in this batch: {unique_labels}\")\n",
    "    print(f\"Minimum label in this batch: {min_label}\")\n",
    "    print(f\"Maximum label in this batch: {max_label}\")\n",
    "    \n",
    "    # Define NUM_CLASSES here for the check\n",
    "    NUM_CLASSES = 4 \n",
    "\n",
    "    if max_label >= NUM_CLASSES or min_label < 0:\n",
    "        print(\"\\n❌ FATAL ERROR: The DataLoader is producing labels that are out of bounds!\")\n",
    "        print(f\"   - Model expects labels from 0 to {NUM_CLASSES-1}.\")\n",
    "        print(f\"   - This batch contains labels from {min_label} to {max_label}.\")\n",
    "    else:\n",
    "        print(\"\\n✅ SUCCESS: The labels in this batch are correct.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred while trying to get a batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50290c6-ae03-4cea-a3d0-4ede3a74bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.5572 | Test Accuracy: 0.8674\n",
      "  -> New best model saved with accuracy: 0.8674\n",
      "Epoch 02/20 | Train Loss: 0.3469 | Test Accuracy: 0.8847\n",
      "  -> New best model saved with accuracy: 0.8847\n",
      "Epoch 03/20 | Train Loss: 0.2894 | Test Accuracy: 0.8957\n",
      "  -> New best model saved with accuracy: 0.8957\n",
      "Epoch 04/20 | Train Loss: 0.2527 | Test Accuracy: 0.9033\n",
      "  -> New best model saved with accuracy: 0.9033\n",
      "Epoch 05/20 | Train Loss: 0.2281 | Test Accuracy: 0.9125\n",
      "  -> New best model saved with accuracy: 0.9125\n",
      "Epoch 06/20 | Train Loss: 0.2082 | Test Accuracy: 0.9076\n",
      "  -> No improvement. Patience: 1/3\n",
      "Epoch 07/20 | Train Loss: 0.1911 | Test Accuracy: 0.9133\n",
      "  -> New best model saved with accuracy: 0.9133\n",
      "Epoch 08/20 | Train Loss: 0.1735 | Test Accuracy: 0.9120\n",
      "  -> No improvement. Patience: 1/3\n",
      "Epoch 09/20 | Train Loss: 0.1610 | Test Accuracy: 0.9146\n",
      "  -> New best model saved with accuracy: 0.9146\n",
      "Epoch 10/20 | Train Loss: 0.1486 | Test Accuracy: 0.9163\n",
      "  -> New best model saved with accuracy: 0.9163\n",
      "Epoch 11/20 | Train Loss: 0.1369 | Test Accuracy: 0.9145\n",
      "  -> No improvement. Patience: 1/3\n",
      "Epoch 12/20 | Train Loss: 0.1268 | Test Accuracy: 0.9151\n",
      "  -> No improvement. Patience: 2/3\n",
      "Epoch 13/20 | Train Loss: 0.1174 | Test Accuracy: 0.9153\n",
      "  -> No improvement. Patience: 3/3\n",
      "\n",
      "EARLY STOPPING after 3 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Supervisor's State for Early Stopping ---\n",
    "epochs_without_improvement = 0\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_tokens, batch_labels in train_dataloader:\n",
    "            batch_tokens = batch_tokens.to(config.DEVICE)\n",
    "            batch_labels = batch_labels.to(config.DEVICE)\n",
    "            \n",
    "            logits = model(batch_tokens)\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #avg training loss\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # --- Evaluation Phase ---\n",
    "    test_accuracy = evaluate(model, test_dataloader, config.DEVICE)\n",
    "    print(f\"Epoch {epoch+1:02d}/{config.NUM_EPOCHS} | Train Loss: {avg_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # --- (EARLY STOPPING) ---\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "        print(f\"  -> New best model saved with accuracy: {best_test_accuracy:.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  -> No improvement. Patience: {epochs_without_improvement}/{config.PATIENCE}\")\n",
    "\n",
    "    # Check if the supervisor should stop the training\n",
    "    if epochs_without_improvement >= config.PATIENCE:\n",
    "        print(f\"\\nEARLY STOPPING after {config.PATIENCE} epochs without improvement.\")\n",
    "        break # Exit the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bde5b5-0754-4f2b-9efd-54f73db0b8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_news",
   "language": "python",
   "name": "transformer_news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
